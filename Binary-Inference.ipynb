{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow import keras\n",
    "#from tensorflow.python.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D\n",
    "#from keras.optimizers import Adam\n",
    "#from Data_Preparation import *\n",
    "#from keras.utils import to_categorical\n",
    "from scipy.spatial import distance_matrix\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "import glob\n",
    "import os\n",
    "from scipy.ndimage import distance_transform_cdt\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "#import matplotlib.image as mpimg\n",
    "#import imutils\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the  Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = keras.models.load_model('//ibs9010/current_data/Data_Keshav/image/Classification_model/NeuronClassifierModels/BinaryClassifier_proper_bigtile_50epoch_trial5data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 70 # fix the size of the image\n",
    "#Path = \"//ibs9010/current_data/Data_Keshav/image/Classification_model/Data/Newtest\"\n",
    "Path = \"//ibs9010/current_data/Data_Keshav/Cropped_Images/S1/\"\n",
    "#Path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/testingcrop/S051/'\n",
    "Testing_image = []# list of testing images\n",
    "image_name = []# list of image names\n",
    "\n",
    "for img in os.listdir(Path): #gives the images from the path\n",
    "    img_array = cv2.imread(os.path.join(Path,img),cv2.IMREAD_GRAYSCALE) # read every image in gray scale from the given path\n",
    "    Testing_image.append(img_array)\n",
    "    image_name.append(img)\n",
    "    \n",
    "print(\"Number of test images:\",len(Testing_image))\n",
    "#Running Inference\n",
    "Test_image = np.array(Testing_image).reshape(-1,img_size,img_size,1)\n",
    "#make an array of every element of list from Testing_image and then reshape them\n",
    "Test_image = Test_image/255.0\n",
    "scores = model.predict_classes(Test_image)\n",
    "\n",
    "# Results\n",
    "neuron_image_counter = bkgd_image_counter = 0\n",
    "for i in range(len(Test_image)):\n",
    "    #print(\"Image:\",image_name[i],\"with score:\",scores[i]) \n",
    "    if scores[i]==0:\n",
    "        bkgd_image_counter +=1\n",
    "    if scores[i]==1:\n",
    "        neuron_image_counter += 1\n",
    "    \n",
    "print(\"Total testing images:\",len(Testing_image))\n",
    "print(\"bkgd_image_counter:\",bkgd_image_counter,\"neuron_image_counter:\",neuron_image_counter)\n",
    "\n",
    "\n",
    "## Saving classified tiles into respective folders\n",
    "\n",
    "dest_path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/Inference/BinaryClasses/singleneurontiles'\n",
    "#dest_path = \"//ibs9010/current_data/Data_Keshav/image/Classification_model/Inference/BinaryClasses/NewTestsingleneurontiles\"\n",
    "directory = '//ibs9010/current_data/Data_Keshav/image/Classification_model/Data/Test/'\n",
    "\n",
    "# Delete any exiting images before saving\n",
    "import glob\n",
    "os.chdir(dest_path)\n",
    "files=glob.glob('*.tif')\n",
    "for filename in files:\n",
    "    os.unlink(filename)\n",
    "    \n",
    "#need the images of class1 to be appended in a separate list\n",
    "single_image_list  = [] # contains list of filenames of single neurons\n",
    "#sn_images = [] # conatins tiles of single neurons #(actual images itself)\n",
    "for k in range(1,len(Test_image)):\n",
    "    if scores[k]==1:\n",
    "        #print(Test_image[i])\n",
    "        #print(\"Image \",image_name[k])\n",
    "        #sn_images.append(Test_image[i])\n",
    "        single_image_list.append(image_name[k])\n",
    "        \n",
    "#print(len(single_image_list))  \n",
    "#image_name[i]\n",
    "for i in range(len(Test_image)):\n",
    "    if scores[i]==1:\n",
    "        #print(\"Image:\",image_name[i])\n",
    "        #print(\"TestImage:\",Test_image[i])\n",
    "        #cv2.imwrite(image_name[i], Test_image[i])\n",
    "        cv2.imwrite(os.path.join(dest_path,image_name[i]), Test_image[i]*255)\n",
    "        #pass\n",
    "        \n",
    "# Generating pickle file For Single neurons\n",
    "import pickle\n",
    "with open('//ibs9010/current_data/Data_Keshav/image/Classification_model/1july_binary_SN_MG48_3day_bs_S051.pkl', 'wb') as f:\n",
    "    pickle.dump(single_image_list, f)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "282\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "#dest_path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/Inference/BinaryClasses/NewTestbackgroundneurontiles'\n",
    "dest_path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/Inference/BinaryClasses/backgroundneurontiles'\n",
    "directory = '//ibs9010/current_data/Data_Keshav/image/Classification_model/Data/Test/'\n",
    "\n",
    "# Delete all exiting images\n",
    "import glob\n",
    "os.chdir(dest_path)\n",
    "files=glob.glob('*.tif')\n",
    "for filename in files:\n",
    "    os.unlink(filename)\n",
    "\n",
    "#need the images of class1 to be appended in a separate list\n",
    "bkgd_image_list  = [] # contains list of filenames of single neurons\n",
    "#sn_images = [] # conatins tiles of single neurons #(actual images itself)\n",
    "for k in range(1,len(Test_image)):\n",
    "    if scores[k]==0:\n",
    "        #print(Test_image[i])\n",
    "        #print(\"Image \",image_name[k])\n",
    "        #sn_images.append(Test_image[i])\n",
    "        bkgd_image_list.append(image_name[k])\n",
    "        \n",
    "print(len(bkgd_image_list))  \n",
    "#image_name[i]\n",
    "for i in range(len(Test_image)):\n",
    "    if scores[i]==0:\n",
    "        #print(\"Image:\",image_name[i])\n",
    "        #print(\"TestImage:\",Test_image[i])\n",
    "        #cv2.imwrite(image_name[i], Test_image[i])\n",
    "        cv2.imwrite(os.path.join(dest_path,image_name[i]), Test_image[i]*255)\n",
    "        #pass\n",
    "'''        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landmarks Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine local coordinates in a tile that has been classified as to having containing Neurons\n",
    "def local_coordinates(file):\n",
    "    x_coordinates = [] #x-coordinate\n",
    "    y_coordinates = [] #y-coordinate\n",
    "    z_coordinates = [] #y-coordinate\n",
    "    coordinates = [] #x,y-coordinate put together\n",
    "    #print(file)\n",
    "    img = cv2.imread(file)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # global thresholding to create binarized image\n",
    "    ret1,th1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n",
    "    # Otsu's thresholding\n",
    "    ret2,th2 = cv2.threshold(img,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "    \n",
    "    #plt.imshow(th2,cmap = \"gray\")\n",
    "    #plt.show()\n",
    "    \n",
    "    #Gaussian blurring to smoothen the image\n",
    "    #blur_image = cv2.GaussianBlur(th2,(5,5),0)\n",
    "    blur_image = cv2.medianBlur(th2,5)\n",
    "    #plt.imshow(blur_image,cmap = \"gray\")\n",
    "    #plt.show()\n",
    "    \n",
    "    #image array\n",
    "    #npimage = np.array(th2)\n",
    "    npimage = np.array(blur_image)\n",
    "    \n",
    "    indsx,indsy = np.where(npimage>0)\n",
    "    x,y = [indsy.mean(),indsx.mean()]\n",
    "    #print(x,y)\n",
    "    Cx,Cy = int(x), int(y)\n",
    "    \n",
    "    cv2.circle(img, (Cx, Cy), 1, (255, 0, 0), 1)\n",
    "   \n",
    "    # show the image\n",
    "    #plt.imshow(img,cmap = \"gray\")\n",
    "    #plt.show()\n",
    "    \n",
    "    return x,y\n",
    "\n",
    "# Path to obatin the classified tiles\n",
    "path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/Inference/BinaryClasses/singleneurontiles/*'\n",
    "output_path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/landmarks/'\n",
    "\n",
    "# Pickle files in order to obtain (from the Cropping function to get) Vertices and tile counter data and also list of neuron tiles after classification\n",
    "with open('//ibs9010/current_data/Data_Keshav/image/Classification_model/11aug_trial5_binary_SN_MG48_3day_bs_S33.pkl','rb') as f:\n",
    "    loaded_obj = pickle.load(f)\n",
    "\n",
    "with open('//ibs9010/current_data/Data_Keshav/image/Classification_model/global_xy_MG48_3day_bs_S33_bigtile_piamasked.pkl','rb') as g:\n",
    "    loaded_data = pickle.load(g)#cropped_data\n",
    "\n",
    "#dest_path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/singleneurontiles/'\n",
    "dest_path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/Inference/BinaryClasses/singleneurontiles/'\n",
    "#dest_path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/Inference/ThreeClasses/singleneurontiles/'\n",
    "global_list = []\n",
    "coordinates = []\n",
    "for item in loaded_obj:\n",
    "    local_x, local_y = local_coordinates(dest_path+item)\n",
    "    filename= item[:-4]\n",
    "    #print(filename)\n",
    "    \n",
    "    num = loaded_data[loaded_data['Tile_Counter'] == int(filename)].index[0]\n",
    "    #num = loaded_data[loaded_data['Correct_Tile_Counter'] == int(filename) ].index[0]\n",
    "    \n",
    "    global_list = loaded_data.at[num, 'Vertices']\n",
    "    #print(num,global_list)\n",
    "    global_x = global_list[3]+  local_x\n",
    "    global_y = global_list[0] + local_y\n",
    "    globals = [global_x,global_y]\n",
    "    coordinates.append(globals)\n",
    "    \n",
    "# Merging landmarks for partials!\n",
    "\n",
    "from math import sqrt\n",
    "templist =[]\n",
    "newcoordinatelist = []\n",
    "\n",
    "for index_p1,p1 in enumerate(coordinates):\n",
    "    for p2 in coordinates[index_p1+1:]:\n",
    "        x2 = p2[0]\n",
    "        x1 = p1[0]\n",
    "        y2 = p2[1]\n",
    "        y1 = p1[1]\n",
    "        #print(x2,x1)\n",
    "        #print(y2,y1)\n",
    "        dist = sqrt((x2 - x1)** 2.0 + (y2 - y1)**2.0)\n",
    "        \n",
    "        if dist <20:\n",
    "            print(dist)\n",
    "            templist.append(p1)\n",
    "            templist.append(p2)\n",
    "            avg = [(p1[0]+ p2[0])/2, (p1[1]+ p2[1])/2]\n",
    "            print(avg)\n",
    "            newcoordinatelist.append(avg)\n",
    "\n",
    "for p in coordinates:\n",
    "    if p not in templist:\n",
    "        newcoordinatelist.append(p)\n",
    "\n",
    "#adding zero for z dimension for landmark files:\n",
    "for item in newcoordinatelist:\n",
    "    item.append(0.5)\n",
    "\n",
    "modified_coordinates= []\n",
    "for new in newcoordinatelist:\n",
    "    x_modified = new[0]*0.868\n",
    "    y_modified = new[1]*0.868\n",
    "    z_modified = new[2]*0.868\n",
    "    modified_coordinates.append([x_modified,y_modified,z_modified])\n",
    "\n",
    "# Function for Creating LandmarkcAscii Files\n",
    "def writeLandmarkCoords(filename, landmarkList):\n",
    "  with open(filename, 'w') as file:\n",
    "     file.write(\"# Avizo 3D ASCII 2.0 \\n\")\n",
    "     file.write(\"define Markers \")\n",
    "     file.write(\"{}\".format(len(landmarkList)))\n",
    "     file.write(\"\\n\")\n",
    "     file.write(\"Parameters {\\n\")\n",
    "     file.write(\"    NumSets 1,\\n\")\n",
    "     file.write(\"    ContentType \\\"LandmarkSet\\\"\\n\")\n",
    "     file.write(\"}\\n\")\n",
    "\n",
    "     file.write(\"Markers { float[3] Coordinates } @1\\n\")\n",
    "\n",
    "     file.write(\"# Data section follows\\n\")\n",
    "     file.write(\"@1\\n\")\n",
    "     for landmark in landmarkList:\n",
    "         file.write(\"{}\".format(landmark[0]))\n",
    "         file.write(\" \")\n",
    "         file.write(\"{}\".format(landmark[1]))\n",
    "         file.write(\" \")\n",
    "         file.write(\"{}\".format(landmark[2]))\n",
    "         file.write(\"\\n\")\n",
    "        \n",
    "#Calling function to create respective landmark file\n",
    "writeLandmarkCoords(output_path+\"11aug_trial5_MG48_binary_S33.landmarkAscii\", modified_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP, FP, FN Calculator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_landmarks(filename,only_2d=True):\n",
    "    manual_landmark_list = []\n",
    "    lines = []\n",
    "    with open(filename, 'r') as csb:\n",
    "        lines = csb.readlines()\n",
    "    number = 0\n",
    "\n",
    "    # for each manual landmark\n",
    "    for line in lines:\n",
    "        #print line\n",
    "        if line.startswith(\"@1\"):\n",
    "            number = 1\n",
    "            continue\n",
    "        if number == 1 and line.isspace() == False:\n",
    "            #print line.split()\n",
    "            pt = list(map(float,line.split()))\n",
    "            #pt = [pt[0]*self.axis_directions[0],pt[1]*self.axis_directions[1],pt[2]*self.axis_directions[2]]\n",
    "            manual_landmark_list.append([pt[0],pt[1]])\n",
    "\n",
    "    return manual_landmark_list\n",
    "\n",
    "def write_landmarks(landmarks,filename):\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(\"# Avizo 3D ASCII 2.0 \\n\")\n",
    "        file.write(\"define Markers \")\n",
    "        file.write(\"{}\".format(len(landmarks)))\n",
    "        file.write(\"\\n\")\n",
    "        file.write(\"Parameters {\\n\")\n",
    "        file.write(\"    NumSets 1,\\n\")\n",
    "        file.write(\"    ContentType \\\"LandmarkSet\\\"\\n\")\n",
    "        file.write(\"}\\n\")\n",
    "\n",
    "        file.write(\"Markers { float[3] Coordinates } @1\\n\")\n",
    "\n",
    "        file.write(\"# Data section follows\\n\")\n",
    "        file.write(\"@1\\n\")\n",
    "\n",
    "        if len(landmarks)!=0:\n",
    "            for landmark in landmarks:\n",
    "                ##print(landmark[0][0])\n",
    "                file.write(\"{}\".format(landmark[0]))\n",
    "                file.write(\" \")\n",
    "                file.write(\"{}\".format(landmark[1]))\n",
    "                file.write(\" \")\n",
    "                if len(landmark) > 2:\n",
    "                    file.write(\"{}\".format(landmark[2]))\n",
    "                else:\n",
    "                    file.write(\"{}\".format(0))\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "#manual_landmarks_file = '/nas1/Data_aman/00_Rabies/MG48_3Day_bs/01_Data/00_Landmark/000_manual/00_original/S051_Landmarks.landmarkAscii'\n",
    "#auto_landmarks_file_full = '/nas1/Data_Mythreya/RabiesCountingProject/22june_MG48_S51_trinary.landmarkAscii' \n",
    "manual_landmarks_file = '//ibs9010/current_data/Data_Keshav/image/Classification_model/manual_landmark_files/S033_Landmarks.landmarkAscii'\n",
    "auto_landmarks_file_full=\"//ibs9010/current_data/Data_Keshav/image/Classification_model/landmarks/11aug_trial5_MG48_binary_S33.landmarkAscii\"\n",
    "\n",
    "\n",
    "#auto_landmarks_file_partial = '/nas1/Data_Mythreya/RabiesCountingProject/MG48_S51-26may-PN.landmarkAscii'\n",
    "error_resolution = 25 # radius around manual landmark where we compare , in microns\n",
    "output_path = '//ibs9010/current_data/Data_Keshav/image/Classification_model/LMfiles/'\n",
    "exp = 'S020'\n",
    "\n",
    "manual_landmarks = read_landmarks(manual_landmarks_file)\n",
    "auto_landmarks_full = read_landmarks(auto_landmarks_file_full)\n",
    "#auto_landmarks_partial = read_landmarks(auto_landmarks_file_partial)\n",
    "\n",
    "auto_landmarks_all = []\n",
    "for pt in auto_landmarks_full:\n",
    "    auto_landmarks_all.append(pt)\n",
    "# for pt in auto_landmarks_partial:\n",
    "#     auto_landmarks_all.append(pt)\n",
    "    \n",
    "manual_np = np.array(manual_landmarks)\n",
    "auto_np = np.array(auto_landmarks_all)\n",
    "\n",
    "\n",
    "confusion_mat = (distance_matrix(manual_np,auto_np) < error_resolution)\n",
    "\n",
    "\n",
    "TP = (confusion_mat.sum(axis=1)>0).sum()\n",
    "FN = (confusion_mat.sum(axis=1)==0).sum()\n",
    "FP = (confusion_mat.sum(axis=0)==0).sum()\n",
    "\n",
    "TP_per = (TP/len(manual_landmarks))*100\n",
    "FN_per = (FN/len(manual_landmarks))*100\n",
    "FP_per = (FP/len(manual_landmarks))*100\n",
    "\n",
    "TP_locations = np.where(confusion_mat.sum(axis=1))\n",
    "FN_locations = np.where(confusion_mat.sum(axis=1)==0)\n",
    "FP_locations = np.where(confusion_mat.sum(axis=0)==0)\n",
    "\n",
    "write_landmarks(manual_np[TP_locations],output_path+exp+'_Matches(TP).landmarksAscii')\n",
    "write_landmarks(manual_np[FN_locations],output_path+exp+'_Misses(FN).landmarksAscii')\n",
    "write_landmarks(auto_np[FP_locations],output_path+exp+'_FP.landmarksAscii')\n",
    "\n",
    "\n",
    "print(\"TP,FN,FP:\",TP,FN,FP)\n",
    "print(\"TP_per,FN_per,FP_per:\",TP_per,FN_per,FP_per)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
